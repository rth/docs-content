---
meta:
  title: LLM Endpoints - Concepts
  description: This page explains all the concepts related to Inference for LLMs
content:
  h1: LLM Endpoints - Concepts
  paragraph: This page explains all the concepts related to Inference for LLMs
tags: 
categories:
  - ia
---

## Allowed IPs

Access Control List (ACL) rules [define permissions for remote access to an Instance](/ai-data/llm-inference/how-to/manage-allowed-ip-addresses/). A rule consists of an IP address or an IP range. You can use them to define which host and networks can connect to your Database Instance's endpoint. You can add, edit, or delete rules from your ACLs. The initial setup of a Database Instance allows full network access from anywhere (`0.0.0.0/0`).

Access control is handled directly at network-level by Load Balancers, making the filtering more efficient, universal and relieving the Database Instance from this task.

## Context size

The **context size** refers to the length or size of the input text used to generate predictions or responses from a large language model. It is crucial in determining the model's understanding of the given prompt or query.

## Deployment

A **deployment** makes a trained language model available for real-world applications. It encompasses tasks such as integrating the model into existing systems, optimizing its performance, and ensuring scalability and reliability.

## Embedding Models

Embedding models are a representation-learning technique that converts textual data into numerical vectors. These vectors capture semantic information about the text and are often used as input to downstream machine-learning models or algorithms.

## Endpoint

An **endpoint** in the context of large language models refers to a network-accessible URL or interface through which clients can interact with the model for inference tasks. It exposes methods for sending input data and receiving model predictions or responses.

## Fine-tuning

Fine-tuning involves further training a pre-trained language model on domain-specific or task-specific data to improve performance on a particular task. This process often includes updating the model's parameters using a smaller, task-specific dataset.

## Few-shot prompting

Few-shot prompting is a technique used to generate responses from a language model using only a few examples or prompts as input. It leverages the model's ability to generalize from limited training data to produce coherent and contextually relevant outputs.


## Inference
Add description

## Large Language Model Applications

Large Language Model Applications are applications or software tools that leverage the capabilities of large language models for various tasks, such as text generation, summarization, or translation. These apps provide user-friendly interfaces for interacting with the models and accessing their functionalities.

## Large Language Models (LLMs)
## Prompt

.. A prompt is
## Quantization

Quantization is a technique used to reduce the precision of numerical values in a model's parameters or activations to improve efficiency and reduce memory footprint during inference. It involves representing floating-point values with fewer bits while minimizing the loss of accuracy.

## Retrieval Augmented Generation (RAG)

Retrieval Augmented Generation (RAG) is a framework combining information retrieval elements with language generation to enhance the capabilities of large language models. It involves retrieving relevant context or knowledge from external sources and incorporating it into the generation process to produce more informative and contextually grounded outputs.

